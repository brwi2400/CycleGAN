{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Project Setup\n##### Description of the problem\nThe goal is to transform images in the style of Claude Monet using Generative Adversarial Networks (GANs). We need to create a system that can transform regular photos into images that look like Monet's painting style. A challenge in this task is maintaining realism while also embedding his style.\n\n\n##### What is a GAN?\nGenerative Adversarial Networks (GANs) are a class of machine learning models where two networks (Generator and a Discriminator) exist. The Generator creates the transformations from the input images and the Discriminator evaluates authenticity. After many epochs of training, the Generator will be able to trick the Discriminator into believing the output images are real, indicating our generator is performing well.\n\n##### Description of the data\nThe dataset includes two folders:\n\n- Photos: A set of realistic images representing the real-world settings.\n- Monet Paintings: A collection of images in Monet’s artistic style. This is the target style.\n","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport time\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:27:33.697292Z","iopub.execute_input":"2024-11-30T21:27:33.697980Z","iopub.status.idle":"2024-11-30T21:27:33.703183Z","shell.execute_reply.started":"2024-11-30T21:27:33.697943Z","shell.execute_reply":"2024-11-30T21:27:33.702288Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"#########################\n# Display Sample Images #\n#########################\nmonet_path = '/kaggle/input/gan-getting-started/monet_jpg'\nphoto_path = '/kaggle/input/gan-getting-started/photo_jpg'\nmonet_files = os.listdir(monet_path)\nphoto_files = os.listdir(photo_path)\n\ndef display_images(img_folder, img_list, title, num_samples=5):\n    plt.figure(figsize=(15, 5))\n    for i, img_name in enumerate(img_list[:num_samples]):\n        img = Image.open(os.path.join(img_folder, img_name))\n        plt.subplot(1, num_samples, i + 1)\n        plt.imshow(img)\n        plt.axis('off')\n        plt.title(f\"{title} {i + 1}\")\n    plt.show()\n\ndisplay_images(monet_path, monet_files, 'Monet Image')\ndisplay_images(photo_path, photo_files, 'Photo Image')\n\n\n#########################\n# Total Number of files #\n#########################\nprint(f\"Number of Monet images: {len(monet_files)}\")\nprint(f\"Number of Photo images: {len(photo_files)}\")\n\n\n###########################\n# Check Image Size & Mode #\n###########################\nsample_img = Image.open(os.path.join(monet_path, monet_files[0]))\nprint(f\"Image size: {sample_img.size}, Mode: {sample_img.mode}\")\n\n\n###############################\n# Pixel Distribution for each #\n###############################\ndef plot_color_histogram(image_path, title):\n    img = Image.open(image_path).convert('RGB')\n    img_array = np.array(img)\n    colors = ('r', 'g', 'b')\n    plt.figure(figsize=(8, 4))\n    for i, color in enumerate(colors):\n        plt.hist(img_array[:, :, i].flatten(), bins=256, color=color, alpha=0.5, label=f'{color.upper()}')\n    plt.title(f'Color Distribution: {title}')\n    plt.xlabel('Pixel Value')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()\n\nplot_color_histogram(os.path.join(monet_path, monet_files[0]), \"Monet Sample\")\nplot_color_histogram(os.path.join(photo_path, photo_files[0]), \"Photo Sample\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:27:33.704870Z","iopub.execute_input":"2024-11-30T21:27:33.705167Z","iopub.status.idle":"2024-11-30T21:27:37.544606Z","shell.execute_reply.started":"2024-11-30T21:27:33.705138Z","shell.execute_reply":"2024-11-30T21:27:37.543762Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Architecture\n##### Plan based on EDA\nSome key observations were made during the exploratory data analysis phase. There are 300 training (Monet) images and 7038 input (photo) images.  Each are already sized appropriately at 256x256 and in RGB formatting.  This simplifies the preprocessing step by lessening the steps needed to take. The color distributions in the Monet images are smooth, bell-shaped distributions.  There is a spike in blue pixels at 0 pixel value (also the case for the input photos).  The input photos have a peak in blue at lower pixel values with a more smooth, almost uniform distribution for green and red.  This indicates a stark difference in image styles considering the actual image subjects are similar.\n\n##### Architecure Overview\n1. Preprocess the Monet images\n    - Normalize Images\n    - Create datasets \n3. Model\n    - Use a CycleGAN model since the images are not paired.\n    - The model architecture includes:\n        - Two generators: One to map photos → Monet and another to map Monet → photos.\n        - Two discriminators: One for distinguishing real Monet images from generated ones and another for photos.\n    - Loss Functions\n        - Adversarial Loss: To ensure realism\n        - Cycle Consistency Loss: To preserve content during style transformations\n        - Identity Loss: To Improve Generator Stability\n    - Optimization: Generators and discriminators trained alternately with Adam optimizers.","metadata":{"execution":{"iopub.status.busy":"2024-11-30T17:34:32.566730Z","iopub.execute_input":"2024-11-30T17:34:32.567142Z","iopub.status.idle":"2024-11-30T17:34:32.597029Z","shell.execute_reply.started":"2024-11-30T17:34:32.567093Z","shell.execute_reply":"2024-11-30T17:34:32.595308Z"}}},{"cell_type":"code","source":"##########################\n# LOAD DATA ##############\n##########################\nmonet_path = \"/kaggle/input/gan-getting-started/monet_jpg\"\nphotos_path = \"/kaggle/input/gan-getting-started/photo_jpg\"\n\nclass img_df(Dataset):\n    def __init__(self, directory, transform=None):\n        self.directory = directory\n        self.transform = transform\n        self.image_files = [\n            os.path.join(directory, file) for file in os.listdir(directory) if file.endswith('.jpg')\n        ]\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image\n\nimage_transforms = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nmonet_dataset = img_df(directory=monet_path, transform=image_transforms)\nphoto_dataset = img_df(directory=photos_path, transform=image_transforms)\nmonet_loader = DataLoader(monet_dataset, batch_size=4, shuffle=True, num_workers=4)\nphoto_loader = DataLoader(photo_dataset, batch_size=4, shuffle=True, num_workers=4)\n\n###########################\n# Generators ##############\n###########################\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(in_channels)\n        self.conv2 = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(in_channels)\n\n    def forward(self, x):\n        residual = x\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += residual\n        return out\n\nclass Generator(nn.Module):\n    def __init__(self, input_channels, output_channels):\n        super(Generator, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(input_channels, 64, kernel_size=7, stride=1, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n        \n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(256) for _ in range(6)]\n        )\n\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, output_channels, kernel_size=7, stride=1, padding=3),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.res_blocks(x)\n        x = self.decoder(x)\n        return x\n\ncompute_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nGenerator_AtoB = Generator(input_channels=3, output_channels=3).to(compute_device)\nGenerator_BtoA = Generator(input_channels=3, output_channels=3).to(compute_device)\n\n\n\n###########################\n# Discriminators ##########\n###########################\nclass Discriminator(nn.Module):\n    def __init__(self, input_channels):\n        super().__init__()\n        self.layers = nn.Sequential(\n            self._disc_block(input_channels, 64, normalization=False),\n            self._disc_block(64, 128),\n            self._disc_block(128, 256),\n            self._disc_block(256, 512, stride=1),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)  # Final layer for output score\n        )\n\n    @staticmethod\n    def _disc_block(in_channels, out_channels, stride=2, normalization=True):\n        \"\"\"A helper function to define a block for the discriminator.\"\"\"\n        layers = [nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=stride, padding=1)]\n        if normalization:\n            layers.append(nn.InstanceNorm2d(out_channels))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.layers(x)\n\nDiscriminator_A = Discriminator(input_channels=3).to(compute_device)\nDiscriminator_B = Discriminator(input_channels=3).to(compute_device)\n\n##############\n# Optimizers #\n##############\n\nadversarial_loss = nn.MSELoss()\ncycle_consistency_loss = nn.L1Loss()\nidentity_mapping_loss = nn.L1Loss()\n\ngen_optimizer = optim.Adam(\n    list(Generator_AtoB.parameters()) + list(Generator_BtoA.parameters()),\n    lr=0.0002,\n    betas=(0.5, 0.999)\n)\n\ndisc_optimizer_A = optim.Adam(\n    Discriminator_A.parameters(), \n    lr=0.0002, \n    betas=(0.5, 0.999)\n)\n\ndisc_optimizer_B = optim.Adam(\n    Discriminator_B.parameters(), \n    lr=0.0002, \n    betas=(0.5, 0.999)\n)","metadata":{"trusted":true,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-11-30T21:27:37.545910Z","iopub.execute_input":"2024-11-30T21:27:37.546267Z","iopub.status.idle":"2024-11-30T21:27:37.784015Z","shell.execute_reply.started":"2024-11-30T21:27:37.546229Z","shell.execute_reply":"2024-11-30T21:27:37.783073Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"#################\n# Training loop #\n#################\n\nepochs = 25\nlosses = defaultdict(list)\n\nstart_time = time.process_time()\n\nfor epoch in range(epochs):\n    epoch_start = time.process_time()\n\n    for idx, (data_A, data_B) in enumerate(zip(photo_loader, monet_loader)):\n        inputs_A = data_A.to(compute_device)\n        inputs_B = data_B.to(compute_device)\n\n        gen_optimizer.zero_grad()\n\n        generated_B = Generator_AtoB(inputs_A)\n        fake_B_preds = Discriminator_B(generated_B)\n        loss_G_AtoB = adversarial_loss(fake_B_preds, torch.ones_like(fake_B_preds))\n\n        generated_A = Generator_BtoA(inputs_B)\n        fake_A_preds = Discriminator_A(generated_A)\n        loss_G_BtoA = adversarial_loss(fake_A_preds, torch.ones_like(fake_A_preds))\n\n        rec_A = Generator_BtoA(generated_B)\n        cycle_loss_A = cycle_consistency_loss(rec_A, inputs_A)\n\n        rec_B = Generator_AtoB(generated_A)\n        cycle_loss_B = cycle_consistency_loss(rec_B, inputs_B)\n\n        id_loss_A = identity_mapping_loss(Generator_BtoA(inputs_A), inputs_A)\n        id_loss_B = identity_mapping_loss(Generator_AtoB(inputs_B), inputs_B)\n\n        total_gen_loss = (\n            loss_G_AtoB + loss_G_BtoA +\n            10.0 * (cycle_loss_A + cycle_loss_B) +\n            5.0 * (id_loss_A + id_loss_B)\n        )\n        total_gen_loss.backward()\n        gen_optimizer.step()\n\n        disc_optimizer_A.zero_grad()\n        real_A_preds = Discriminator_A(inputs_A)\n        real_loss_A = adversarial_loss(real_A_preds, torch.ones_like(real_A_preds))\n\n        fake_A_preds = Discriminator_A(generated_A.detach())\n        fake_loss_A = adversarial_loss(fake_A_preds, torch.zeros_like(fake_A_preds))\n\n        disc_A_loss = (real_loss_A + fake_loss_A) * 0.5\n        disc_A_loss.backward()\n        disc_optimizer_A.step()\n\n        disc_optimizer_B.zero_grad()\n        real_B_preds = Discriminator_B(inputs_B)\n        real_loss_B = adversarial_loss(real_B_preds, torch.ones_like(real_B_preds))\n\n        fake_B_preds = Discriminator_B(generated_B.detach())\n        fake_loss_B = adversarial_loss(fake_B_preds, torch.zeros_like(fake_B_preds))\n\n        disc_B_loss = (real_loss_B + fake_loss_B) * 0.5\n        disc_B_loss.backward()\n        disc_optimizer_B.step()\n\n        losses['generator'].append(total_gen_loss.item())\n        losses['disc_A'].append(disc_A_loss.item())\n        losses['disc_B'].append(disc_B_loss.item())\n\n        if idx % 50 == 0:\n            print(f\"Epoch {epoch+1}/{epochs}, Batch {idx}: \"\n                  f\"Gen Loss: {total_gen_loss.item()}, Disc A Loss: {disc_A_loss.item()}, Disc B Loss: {disc_B_loss.item()}\")\n\n    epoch_end = time.process_time()\n    print(f\"Epoch {epoch+1} finished in {epoch_end - epoch_start:.2f}s\")\n\ntotal_end = time.process_time()\nprint(f\"Total training time: {total_end - start_time:.2f}s\")\n\ntorch.save(Generator_AtoB.state_dict(), 'Generator_AtoB.pth')\ntorch.save(Generator_BtoA.state_dict(), 'Generator_BtoA.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T21:27:37.785398Z","iopub.execute_input":"2024-11-30T21:27:37.785702Z","iopub.status.idle":"2024-11-30T22:17:24.407023Z","shell.execute_reply.started":"2024-11-30T21:27:37.785675Z","shell.execute_reply":"2024-11-30T22:17:24.405917Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Results","metadata":{}},{"cell_type":"code","source":"###########################\n# Load and process images #\n###########################\n\nphotos_path = \"/kaggle/input/gan-getting-started/photo_jpg\"\n\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nphoto_filenames = [f for f in os.listdir(photos_path) if f.endswith('.jpg')]\nphotos = [transform(Image.open(os.path.join(photos_path, filename)).convert('RGB')) for filename in photo_filenames]\nphotos = torch.stack(photos)\n\n\n###############################\n# Generate Monet-style images #\n###############################\n\nGenerator_AtoB = Generator(input_channels=3, output_channels=3).to(compute_device)\nGenerator_AtoB.load_state_dict(torch.load('/kaggle/working/Generator_AtoB.pth', map_location=compute_device, weights_only=True))\nGenerator_AtoB.eval()\n\nstart_time = time.time()\n\noutput_dir = '/images'\nos.makedirs(output_dir, exist_ok=True)\n\nwith torch.no_grad():\n    for i, photo in enumerate(photos):\n        photo = photo.unsqueeze(0).to(compute_device)\n        monet_style_image = Generator_AtoB(photo).squeeze(0).cpu()\n        monet_style_image = transforms.ToPILImage()(monet_style_image * 0.5 + 0.5)\n        monet_style_image.save(os.path.join(output_dir, f'monet_style_{i+1}.jpg'))\n\nshutil.make_archive(\"/kaggle/working/images\", 'zip', output_dir)\n\nelapsed_time = time.time() - start_time\nprint(f\"Execution Time: {elapsed_time:.2f} seconds\")\n\n\n######################################\n# Show example image transformations #\n######################################\n\nnum_examples = 5\nfig, axes = plt.subplots(num_examples, 2, figsize=(10, 15))\n\nfor i in range(num_examples):\n    photo = photos[i].unsqueeze(0).to(compute_device)\n    monet_style_image = Generator_AtoB(photo).squeeze(0).cpu()\n    monet_style_image = transforms.ToPILImage()(monet_style_image * 0.5 + 0.5)\n    \n    # Original photo\n    axes[i, 0].imshow(transforms.ToPILImage()(photos[i] * 0.5 + 0.5))\n    axes[i, 0].set_title(\"Original Photo\")\n    axes[i, 0].axis('off')\n    \n    # Monet-style transformation\n    axes[i, 1].imshow(monet_style_image)\n    axes[i, 1].set_title(\"Monet-style Image\")\n    axes[i, 1].axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T22:17:24.409368Z","iopub.execute_input":"2024-11-30T22:17:24.409678Z","iopub.status.idle":"2024-11-30T22:20:51.497819Z","shell.execute_reply.started":"2024-11-30T22:17:24.409650Z","shell.execute_reply":"2024-11-30T22:20:51.496601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\n\nI ultimately scored a 72.  Looking at the output it was a little fuzzy when comparing the trasnformed images to the original vs what the actual Monet paintings looked like.  I suspect I might have gotten better performance if I increased my epoch count from 25 to a much higher value.  I ultimately stuck with a lower value since we are capped on how much GPU allocation we get and development can take quite a bit of it.  The loss scores seemed to plateau after about 10 epochs but I remain suspicious that I could improve the model with more epochs. I may try it for fun once my utilization resets.","metadata":{}}]}